# -*- coding: utf-8 -*-
"""million_headlines

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FVwnAr_6D1NF1Di07pvhwXVGFejGwDLY

**Connecting google colab to google drive and loading the dataset into colab**
"""
import numpy as np
import pandas as pdp
import sklearn
import matplotlib.pyplot as plt


from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('drive/My Drive/Data_Mining/uci-news-aggregator.csv', error_bad_lines=False)
df.head(5)

df_text = df[['TITLE', 'CATEGORY']]

df_text['index'] = df_text.index
documents = df_text
print(len(documents))

"""**Removing the publish date and then added index into dataset which is serial number**"""

documents.head(5)

print(documents.groupby('CATEGORY').size())
print("unique targets: " +documents.CATEGORY.unique())

documents['CATEGORY'] = df.CATEGORY.map({'b':0,'e':1,'m':2,'t':3})
outcomes = documents['CATEGORY']

"""**Performing data preprocessing**


*   Tokenization - splits the text into sentences and sentences into words


*   Lower case and remove punctuation
*    remove words that have fewer than 3 characters


*   remove stopwords

*   Lemmatization - words are lemmatized, which is third person are changed to single person and verbs in future and past are changed into present.
*  Stemming - words are reduced to its stem/root.

Loading genism and nltk libraries
"""

import gensim
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS
import nltk
from nltk.stem import WordNetLemmatizer, SnowballStemmer
from nltk.stem.porter import *
np.random.seed(2018)
nltk.download('wordnet')

"""**function to perform lemmatize and stem preprocessing steps on the data set.**"""

#function to perform lemmatize and stem preprocessing steps on the data set.
stemmer = PorterStemmer()

def lemmatize_stemming(text):
    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))
  
def preprocess(text):
    clean_words = [lemmatize_stemming(token) for token in gensim.utils.simple_preprocess(text) if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3]
    return ' '.join(clean_words)

#Selecting documents to preview after preprocessing
doc_sample = documents[documents['index'] == 0].values[0][0]
print('original question: ')
words = []
for word in doc_sample.split(' '):
    words.append(word)
print(words)
print('\n\n tokenized and lemmatized question: ')
print(preprocess(doc_sample))

"""**Selecting documents to preview after preprocessing**

**Preprocess the entire headline_text**
"""

processed_docs = documents['TITLE'].map(preprocess)

processed_docs.head(5)

"""**Implementing bag of words in sklearn using countvectorizer**"""

#Document term matrix
from sklearn.feature_extraction.text import CountVectorizer
# Instantiate the CountVectorizer method
count_vector = CountVectorizer()
print(count_vector)

"""**Training and testing split**"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(processed_docs,outcomes, random_state=1)

print('Number of rows in the total set: {}'.format(documents.shape[0]))
print('Number of rows in the training set: {}'.format(X_train.shape[0]))
print('Number of rows in the test set: {}'.format(X_test.shape[0]))

"""**fiiting and transforming the training and testing set**"""

# Fit the training data and then return the matrix
x_training_data = count_vector.fit_transform(X_train)
#count_vector.get_feature_names()
# Transform testing data and return the matrix. Note we are not fitting the testing data into the CountVectorizer()
testing_data = count_vector.transform(X_test)

"""**training and testing the data**"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer, precision_score, recall_score, f1_score
rfc = RandomForestClassifier()
params = {'max_depth': range(1,10), 'criterion': ['gini', 'entropy'], 'n_estimators': [5,10,15,20], 'min_samples_leaf': range(1,10), 'min_samples_split': [2,5,10]}
#precision = precision_score(average=None)
#recall = recall_score(average=None)
#f1 = f1_score(average=None)
#scorer = ['precision', 'recall', 'f1']
scorer = make_scorer(f1_score, average=None)
grid_obj = GridSearchCV(rfc,params,scoring = scorer, refit ='f1')
grid_fit = grid_obj.fit(x_training_data, y_train)
best_clf = grid_fit.best_estimator_
best_clf
